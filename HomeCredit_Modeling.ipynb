{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Home Credit Default Risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Methods\n",
    "The dataset is inbalanced. Model performance could improve with different sampling techniques. We will test oversamping, SMOTE (Synthetic Minority Oversampling Technique), and undersampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegressionCV, LassoCV, RidgeCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = data['TARGET']\n",
    "x = data.drop('TARGET', axis = 1)\n",
    "\n",
    "# Create train and test sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampling datasets to train the models with.\n",
    "o_sam = imblearn.over_sampling.RandomOverSampler(sampling_strategy = 0.5, random_state= 123)\n",
    "u_sam = imblearn.under_sampling.RandomUnderSampler(sampling_strategy = 0.5, random_state= 123)\n",
    "smote = imblearn.over_sampling.SMOTE(random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_over , y_over = o_sam.fit_resample(x_train, y_train)\n",
    "x_under, y_under = u_sam.fit_resample(x_train, y_train)\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "We will be testing a total of five model approaches for our analysis. Three of the models will be linear (LogisticRegression, Lasso, & Ridge). Additional models will include, Random Forest Classifier, and K nearest neighbors (KNN). We will use k-fold cross validation for trianing of each of the models. SVM was also considered for the analysis, but removed due to computational time contraints. In total we tested 10 models ranging in methods and specific parameters. The goal is to obatain a model with the best predictive power on the dataset. Each of the members of are team focussed on development of one of the models. Pankhuri - RandomForest, Hasitha - KNN, Meghana - SVC, and Heber - (linear models & xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up kfold cross validation. There will be 10 folds in the cross validation.\n",
    "kfold = KFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Linear models being tested inclide logistic, lasso, and Ridge regression.\n",
    "log_mod = LogisticRegressionCV(cv=kfold, random_state = 123)\n",
    "lasso_mod = LassoCV(cv= kfold, random_state = 123)\n",
    "ridge_mod = RidgeCV(cv= kfold)\n",
    "# SVC is a very resource instensive model. These models were Limited due to time restrictions.\n",
    "SVC_mod1 = SVC(random_state = 123)\n",
    "SVC_mod2 = SVC(kernel = 'linear', random_state = 123)\n",
    "# We will be testing randomforst models with the default parameters and one model with a restricted depth and min_leaf size.\n",
    "rf_mod1 = RandomForestClassifier()\n",
    "rf_mod2 = RandomForestClassifier(max_depth = 5, min_samples_leaf = 500)\n",
    "# Adding an xgboosted model for fun default and set number of estimators.\n",
    "xgb_mod1 = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "xgb_mod2 = xgb.XGBClassifier(n_estimators= 100, objective = 'binary:logistic')\n",
    "# We will run 3 knn models with different values for n_neighbors.\n",
    "knn_mod1 = KNeighborsClassifier()\n",
    "knn_mod2 = KNeighborsClassifier(n_neighbors = 10)\n",
    "knn_mod3 = KNeighborsClassifier(n_neighbors = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelperformance(model, xtrain, ytrain, xtest, ytest):\n",
    "    train = model.fit(xtrain, ytrain)\n",
    "    pred = model.predict(xtest)\n",
    "    if pred[0] != 0 or pred[0] != 1:\n",
    "        pred = [int(i > 0.5) for i in pred] # binary values set to 1 if greater than 0.5\n",
    "    \n",
    "    print('Model')\n",
    "    print(metrics.confusion_matrix(ytest, pred))\n",
    "    print()\n",
    "    print('AUC:', round(metrics.roc_auc_score(ytest, pred), 4))\n",
    "    print(metrics.classification_report(ytest, pred, digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[56475    13]\n",
      " [ 4999    16]]\n",
      "\n",
      "AUC: 0.5015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9187    0.9998    0.9575     56488\n",
      "         1.0     0.5517    0.0032    0.0063      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.7352    0.5015    0.4819     61503\n",
      "weighted avg     0.8888    0.9185    0.8800     61503\n",
      "\n",
      "Model\n",
      "[[56424    64]\n",
      " [ 4948    67]]\n",
      "\n",
      "AUC: 0.5061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9194    0.9989    0.9575     56488\n",
      "         1.0     0.5115    0.0134    0.0260      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.7154    0.5061    0.4918     61503\n",
      "weighted avg     0.8861    0.9185    0.8815     61503\n",
      "\n",
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[56352   136]\n",
      " [ 4868   147]]\n",
      "\n",
      "AUC: 0.5135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9205    0.9976    0.9575     56488\n",
      "         1.0     0.5194    0.0293    0.0555      5015\n",
      "\n",
      "    accuracy                         0.9186     61503\n",
      "   macro avg     0.7200    0.5135    0.5065     61503\n",
      "weighted avg     0.8878    0.9186    0.8839     61503\n",
      "\n",
      "Model\n",
      "[[56352   136]\n",
      " [ 4868   147]]\n",
      "\n",
      "AUC: 0.5135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9205    0.9976    0.9575     56488\n",
      "         1.0     0.5194    0.0293    0.0555      5015\n",
      "\n",
      "    accuracy                         0.9186     61503\n",
      "   macro avg     0.7200    0.5135    0.5065     61503\n",
      "weighted avg     0.8878    0.9186    0.8839     61503\n",
      "\n",
      "Model\n",
      "[[56219   269]\n",
      " [ 4963    52]]\n",
      "\n",
      "AUC: 0.5028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9189    0.9952    0.9555     56488\n",
      "         1.0     0.1620    0.0104    0.0195      5015\n",
      "\n",
      "    accuracy                         0.9149     61503\n",
      "   macro avg     0.5404    0.5028    0.4875     61503\n",
      "weighted avg     0.8572    0.9149    0.8792     61503\n",
      "\n",
      "Model\n",
      "[[56483     5]\n",
      " [ 5013     2]]\n",
      "\n",
      "AUC: 0.5002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    0.9999    0.9575     56488\n",
      "         1.0     0.2857    0.0004    0.0008      5015\n",
      "\n",
      "    accuracy                         0.9184     61503\n",
      "   macro avg     0.6021    0.5002    0.4791     61503\n",
      "weighted avg     0.8669    0.9184    0.8795     61503\n",
      "\n",
      "Model\n",
      "[[56123   365]\n",
      " [ 4948    67]]\n",
      "\n",
      "AUC: 0.5034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9190    0.9935    0.9548     56488\n",
      "         1.0     0.1551    0.0134    0.0246      5015\n",
      "\n",
      "    accuracy                         0.9136     61503\n",
      "   macro avg     0.5370    0.5034    0.4897     61503\n",
      "weighted avg     0.8567    0.9136    0.8790     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model on imb dataset.\n",
    "modelperformance(log_mod, x_train, y_train, x_test, y_test)\n",
    "modelperformance(lasso_mod, x_train, y_train, x_test, y_test)\n",
    "modelperformance(ridge_mod, x_train, y_train, x_test, y_test)\n",
    "# modelperformance(SVC_mod1, x_train, y_train, x_test, y_test)\n",
    "# modelperformance(SVC_mod2, x_train, y_train, x_test, y_test)\n",
    "modelperformance(rf_mod1, x_train, y_train, x_test, y_test)\n",
    "modelperformance(rf_mod2, x_train, y_train, x_test, y_test)\n",
    "modelperformance(xgb_mod1, x_train, y_train, x_test, y_test)\n",
    "modelperformance(xgb_mod2, x_train, y_train, x_test, y_test)\n",
    "modelperformance(knn_mod1, x_train, y_train, x_test, y_test)\n",
    "modelperformance(knn_mod2, x_train, y_train, x_test, y_test)\n",
    "modelperformance(knn_mod3, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three models predicted every application as non-default. These models are no better then using the majority class. This is to be expected with using the imbablanced dataset. Some of the more advanced models did a better job, but we can improve our results with different sampling technique. The highest AUC value was 0.5135 using the XGBoost model.\n",
    "\n",
    "## Oversampling Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[50194  6294]\n",
      " [ 3016  1999]]\n",
      "\n",
      "AUC: 0.6436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9433    0.8886    0.9151     56488\n",
      "         1.0     0.2410    0.3986    0.3004      5015\n",
      "\n",
      "    accuracy                         0.8486     61503\n",
      "   macro avg     0.5922    0.6436    0.6078     61503\n",
      "weighted avg     0.8861    0.8486    0.8650     61503\n",
      "\n",
      "Model\n",
      "[[56204   284]\n",
      " [ 4807   208]]\n",
      "\n",
      "AUC: 0.5182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9212    0.9950    0.9567     56488\n",
      "         1.0     0.4228    0.0415    0.0755      5015\n",
      "\n",
      "    accuracy                         0.9172     61503\n",
      "   macro avg     0.6720    0.5182    0.5161     61503\n",
      "weighted avg     0.8806    0.9172    0.8848     61503\n",
      "\n",
      "Model\n",
      "[[50987  5501]\n",
      " [ 3329  1686]]\n",
      "\n",
      "AUC: 0.6194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9387    0.9026    0.9203     56488\n",
      "         1.0     0.2346    0.3362    0.2763      5015\n",
      "\n",
      "    accuracy                         0.8564     61503\n",
      "   macro avg     0.5867    0.6194    0.5983     61503\n",
      "weighted avg     0.8813    0.8564    0.8678     61503\n",
      "\n",
      "Model\n",
      "[[50165  6323]\n",
      " [ 2972  2043]]\n",
      "\n",
      "AUC: 0.6477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9441    0.8881    0.9152     56488\n",
      "         1.0     0.2442    0.4074    0.3054      5015\n",
      "\n",
      "    accuracy                         0.8489     61503\n",
      "   macro avg     0.5941    0.6477    0.6103     61503\n",
      "weighted avg     0.8870    0.8489    0.8655     61503\n",
      "\n",
      "Model\n",
      "[[50165  6323]\n",
      " [ 2972  2043]]\n",
      "\n",
      "AUC: 0.6477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9441    0.8881    0.9152     56488\n",
      "         1.0     0.2442    0.4074    0.3054      5015\n",
      "\n",
      "    accuracy                         0.8489     61503\n",
      "   macro avg     0.5941    0.6477    0.6103     61503\n",
      "weighted avg     0.8870    0.8489    0.8655     61503\n",
      "\n",
      "Model\n",
      "[[46050 10438]\n",
      " [ 3657  1358]]\n",
      "\n",
      "AUC: 0.543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9264    0.8152    0.8673     56488\n",
      "         1.0     0.1151    0.2708    0.1616      5015\n",
      "\n",
      "    accuracy                         0.7708     61503\n",
      "   macro avg     0.5208    0.5430    0.5144     61503\n",
      "weighted avg     0.8603    0.7708    0.8097     61503\n",
      "\n",
      "Model\n",
      "[[46379 10109]\n",
      " [ 3745  1270]]\n",
      "\n",
      "AUC: 0.5371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9253    0.8210    0.8701     56488\n",
      "         1.0     0.1116    0.2532    0.1549      5015\n",
      "\n",
      "    accuracy                         0.7747     61503\n",
      "   macro avg     0.5184    0.5371    0.5125     61503\n",
      "weighted avg     0.8589    0.7747    0.8117     61503\n",
      "\n",
      "Model\n",
      "[[52481  4007]\n",
      " [ 4455   560]]\n",
      "\n",
      "AUC: 0.5204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9218    0.9291    0.9254     56488\n",
      "         1.0     0.1226    0.1117    0.1169      5015\n",
      "\n",
      "    accuracy                         0.8624     61503\n",
      "   macro avg     0.5222    0.5204    0.5211     61503\n",
      "weighted avg     0.8566    0.8624    0.8595     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training models on Oversampled data\n",
    "modelperformance(log_mod, x_over, y_over, x_test, y_test)\n",
    "modelperformance(lasso_mod, x_over, y_over, x_test, y_test)\n",
    "modelperformance(ridge_mod, x_over, y_over, x_test, y_test)\n",
    "# modelperformance(SVC_mod1, x_over, y_over, x_test, y_test)\n",
    "# modelperformance(SVC_mod2, x_over, y_over, x_test, y_test)\n",
    "modelperformance(rf_mod1, x_over, y_over, x_test, y_test)\n",
    "modelperformance(rf_mod2, x_over, y_over, x_test, y_test)\n",
    "modelperformance(xgb_mod1, x_over, y_over, x_test, y_test)\n",
    "modelperformance(xgb_mod2, x_over, y_over, x_test, y_test)\n",
    "modelperformance(knn_mod1, x_over, y_over, x_test, y_test)\n",
    "modelperformance(knn_mod2, x_over, y_over, x_test, y_test)\n",
    "modelperformance(knn_mod3, x_over, y_over, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling has made a impact on model performance. The highest AUC value was 0.6477, using the xgboost model. Changing the parameters in the xgboosted model made no significant impact on the results of the accuracy.\n",
    "\n",
    "## Undersampling Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[56488     0]\n",
      " [ 5015     0]]\n",
      "\n",
      "AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9185    1.0000    0.9575     56488\n",
      "         1.0     0.0000    0.0000    0.0000      5015\n",
      "\n",
      "    accuracy                         0.9185     61503\n",
      "   macro avg     0.4592    0.5000    0.4787     61503\n",
      "weighted avg     0.8436    0.9185    0.8794     61503\n",
      "\n",
      "Model\n",
      "[[50169  6319]\n",
      " [ 3033  1982]]\n",
      "\n",
      "AUC: 0.6417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9430    0.8881    0.9147     56488\n",
      "         1.0     0.2388    0.3952    0.2977      5015\n",
      "\n",
      "    accuracy                         0.8479     61503\n",
      "   macro avg     0.5909    0.6417    0.6062     61503\n",
      "weighted avg     0.8856    0.8479    0.8644     61503\n",
      "\n",
      "Model\n",
      "[[50006  6482]\n",
      " [ 3095  1920]]\n",
      "\n",
      "AUC: 0.6341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9417    0.8852    0.9126     56488\n",
      "         1.0     0.2285    0.3829    0.2862      5015\n",
      "\n",
      "    accuracy                         0.8443     61503\n",
      "   macro avg     0.5851    0.6341    0.5994     61503\n",
      "weighted avg     0.8836    0.8443    0.8615     61503\n",
      "\n",
      "Model\n",
      "[[50953  5535]\n",
      " [ 3311  1704]]\n",
      "\n",
      "AUC: 0.6209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9390    0.9020    0.9201     56488\n",
      "         1.0     0.2354    0.3398    0.2781      5015\n",
      "\n",
      "    accuracy                         0.8562     61503\n",
      "   macro avg     0.5872    0.6209    0.5991     61503\n",
      "weighted avg     0.8816    0.8562    0.8678     61503\n",
      "\n",
      "Model\n",
      "[[48639  7849]\n",
      " [ 2758  2257]]\n",
      "\n",
      "AUC: 0.6555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9463    0.8611    0.9017     56488\n",
      "         1.0     0.2233    0.4500    0.2985      5015\n",
      "\n",
      "    accuracy                         0.8275     61503\n",
      "   macro avg     0.5848    0.6555    0.6001     61503\n",
      "weighted avg     0.8874    0.8275    0.8525     61503\n",
      "\n",
      "Model\n",
      "[[48639  7849]\n",
      " [ 2758  2257]]\n",
      "\n",
      "AUC: 0.6555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9463    0.8611    0.9017     56488\n",
      "         1.0     0.2233    0.4500    0.2985      5015\n",
      "\n",
      "    accuracy                         0.8275     61503\n",
      "   macro avg     0.5848    0.6555    0.6001     61503\n",
      "weighted avg     0.8874    0.8275    0.8525     61503\n",
      "\n",
      "Model\n",
      "[[46046 10442]\n",
      " [ 3749  1266]]\n",
      "\n",
      "AUC: 0.5338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9247    0.8151    0.8665     56488\n",
      "         1.0     0.1081    0.2524    0.1514      5015\n",
      "\n",
      "    accuracy                         0.7693     61503\n",
      "   macro avg     0.5164    0.5338    0.5089     61503\n",
      "weighted avg     0.8581    0.7693    0.8082     61503\n",
      "\n",
      "Model\n",
      "[[52404  4084]\n",
      " [ 4459   556]]\n",
      "\n",
      "AUC: 0.5193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9216    0.9277    0.9246     56488\n",
      "         1.0     0.1198    0.1109    0.1152      5015\n",
      "\n",
      "    accuracy                         0.8611     61503\n",
      "   macro avg     0.5207    0.5193    0.5199     61503\n",
      "weighted avg     0.8562    0.8611    0.8586     61503\n",
      "\n",
      "Model\n",
      "[[51201  5287]\n",
      " [ 4343   672]]\n",
      "\n",
      "AUC: 0.5202\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9218    0.9064    0.9140     56488\n",
      "         1.0     0.1128    0.1340    0.1225      5015\n",
      "\n",
      "    accuracy                         0.8434     61503\n",
      "   macro avg     0.5173    0.5202    0.5183     61503\n",
      "weighted avg     0.8558    0.8434    0.8495     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training models on Undersampled data\n",
    "modelperformance(log_mod, x_under, y_under, x_test, y_test)\n",
    "modelperformance(lasso_mod, x_under, y_under, x_test, y_test)\n",
    "modelperformance(ridge_mod, x_under, y_under, x_test, y_test)\n",
    "# modelperformance(SVC_mod1, x_under, y_under, x_test, y_test)\n",
    "# modelperformance(SVC_mod2, x_under, y_under, x_test, y_test)\n",
    "modelperformance(rf_mod1, x_under, y_under, x_test, y_test)\n",
    "modelperformance(rf_mod2, x_under, y_under, x_test, y_test)\n",
    "modelperformance(xgb_mod1, x_under, y_under, x_test, y_test)\n",
    "modelperformance(xgb_mod2, x_under, y_under, x_test, y_test)\n",
    "modelperformance(knn_mod1, x_under, y_under, x_test, y_test)\n",
    "modelperformance(knn_mod2, x_under, y_under, x_test, y_test)\n",
    "modelperformance(knn_mod3, x_under, y_under, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest AUC value for undersampling data is 0.6555, using the xgboost model.\n",
    "\n",
    "## SMOTE Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "[[46231 10257]\n",
      " [ 2773  2242]]\n",
      "\n",
      "AUC: 0.6327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9434    0.8184    0.8765     56488\n",
      "         1.0     0.1794    0.4471    0.2560      5015\n",
      "\n",
      "    accuracy                         0.7881     61503\n",
      "   macro avg     0.5614    0.6327    0.5663     61503\n",
      "weighted avg     0.8811    0.7881    0.8259     61503\n",
      "\n",
      "Model\n",
      "[[25093 31395]\n",
      " [ 1681  3334]]\n",
      "\n",
      "AUC: 0.5545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9372    0.4442    0.6027     56488\n",
      "         1.0     0.0960    0.6648    0.1678      5015\n",
      "\n",
      "    accuracy                         0.4622     61503\n",
      "   macro avg     0.5166    0.5545    0.3853     61503\n",
      "weighted avg     0.8686    0.4622    0.5673     61503\n",
      "\n",
      "Model\n",
      "[[39233 17255]\n",
      " [ 1635  3380]]\n",
      "\n",
      "AUC: 0.6843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9600    0.6945    0.8060     56488\n",
      "         1.0     0.1638    0.6740    0.2635      5015\n",
      "\n",
      "    accuracy                         0.6929     61503\n",
      "   macro avg     0.5619    0.6843    0.5348     61503\n",
      "weighted avg     0.8951    0.6929    0.7617     61503\n",
      "\n",
      "Model\n",
      "[[56460    28]\n",
      " [ 4981    34]]\n",
      "\n",
      "AUC: 0.5031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9189    0.9995    0.9575     56488\n",
      "         1.0     0.5484    0.0068    0.0134      5015\n",
      "\n",
      "    accuracy                         0.9186     61503\n",
      "   macro avg     0.7337    0.5031    0.4855     61503\n",
      "weighted avg     0.8887    0.9186    0.8805     61503\n",
      "\n",
      "Model\n",
      "[[48782  7706]\n",
      " [ 3332  1683]]\n",
      "\n",
      "AUC: 0.5996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9361    0.8636    0.8984     56488\n",
      "         1.0     0.1793    0.3356    0.2337      5015\n",
      "\n",
      "    accuracy                         0.8205     61503\n",
      "   macro avg     0.5577    0.5996    0.5660     61503\n",
      "weighted avg     0.8744    0.8205    0.8442     61503\n",
      "\n",
      "Model\n",
      "[[56311   177]\n",
      " [ 4878   137]]\n",
      "\n",
      "AUC: 0.5121\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9203    0.9969    0.9570     56488\n",
      "         1.0     0.4363    0.0273    0.0514      5015\n",
      "\n",
      "    accuracy                         0.9178     61503\n",
      "   macro avg     0.6783    0.5121    0.5042     61503\n",
      "weighted avg     0.8808    0.9178    0.8832     61503\n",
      "\n",
      "Model\n",
      "[[56311   177]\n",
      " [ 4878   137]]\n",
      "\n",
      "AUC: 0.5121\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9203    0.9969    0.9570     56488\n",
      "         1.0     0.4363    0.0273    0.0514      5015\n",
      "\n",
      "    accuracy                         0.9178     61503\n",
      "   macro avg     0.6783    0.5121    0.5042     61503\n",
      "weighted avg     0.8808    0.9178    0.8832     61503\n",
      "\n",
      "Model\n",
      "[[48703  7785]\n",
      " [ 3916  1099]]\n",
      "\n",
      "AUC: 0.5407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9256    0.8622    0.8928     56488\n",
      "         1.0     0.1237    0.2191    0.1581      5015\n",
      "\n",
      "    accuracy                         0.8097     61503\n",
      "   macro avg     0.5246    0.5407    0.5254     61503\n",
      "weighted avg     0.8602    0.8097    0.8329     61503\n",
      "\n",
      "Model\n",
      "[[49479  7009]\n",
      " [ 4003  1012]]\n",
      "\n",
      "AUC: 0.5389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9252    0.8759    0.8999     56488\n",
      "         1.0     0.1262    0.2018    0.1553      5015\n",
      "\n",
      "    accuracy                         0.8210     61503\n",
      "   macro avg     0.5257    0.5389    0.5276     61503\n",
      "weighted avg     0.8600    0.8210    0.8391     61503\n",
      "\n",
      "Model\n",
      "[[52622  3866]\n",
      " [ 4465   550]]\n",
      "\n",
      "AUC: 0.5206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9218    0.9316    0.9266     56488\n",
      "         1.0     0.1245    0.1097    0.1166      5015\n",
      "\n",
      "    accuracy                         0.8645     61503\n",
      "   macro avg     0.5232    0.5206    0.5216     61503\n",
      "weighted avg     0.8568    0.8645    0.8606     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training models on SMOTE sampled data\n",
    "modelperformance(log_mod, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(lasso_mod, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(ridge_mod, x_smote, y_smote, x_test, y_test)\n",
    "# modelperformance(SVC_mod1, x_smote, y_smote, x_test, y_test)\n",
    "# modelperformance(SVC_mod2, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(rf_mod1, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(rf_mod2, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(xgb_mod1, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(xgb_mod2, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(knn_mod1, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(knn_mod2, x_smote, y_smote, x_test, y_test)\n",
    "modelperformance(knn_mod3, x_smote, y_smote, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest AUC value for SMOTE data is 0.6843, using a Ridge Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection:\n",
    "Based on the training results, the model we would want to test for the kaggle competition would be the Ridge model trained on the SMOTE data. It had the highest AUC score of 0.6843. AUC was chosen for the best selecter due to the imbalanced data. We wanted a model to be able to identify the applications likely to default and not just correctly identify the applications that would not default. The rest of the sampling tests showed that the xgboost performed as the top model. We may want to consider combining the two estimates to see if they provide a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelperformancenewdata(model, xtrain, ytrain, xtest, ytest, newdata):\n",
    "    train = model.fit(xtrain, ytrain)\n",
    "    y_pred = model.predict(xtest)\n",
    "    pred = model.predict(newdata)\n",
    "    \n",
    "    if y_pred[0] != 0 or y_pred[0] != 1:\n",
    "        y_pred = [int(i > 0.5) for i in y_pred] # binary values set to 1 if greater than 0.5\n",
    "    \n",
    "    if pred[0] != 0 or pred[0] != 1:\n",
    "        pred = [int(i > 0.5) for i in pred] # binary values set to 1 if greater than 0.5\n",
    "        \n",
    "     # Calculate other evaluation metrics\n",
    "    precision = precision_score(ytest, y_pred)\n",
    "    recall = recall_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred)\n",
    "    auc_score =  round(metrics.roc_auc_score(ytest, y_pred), 4)\n",
    "    accuracy = round(metrics.accuracy_score(y_test, y_pred), 4)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Prediction Accuracy:\", accuracy)\n",
    "    print(\"AUC Score:\", auc_score)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.6929\n",
      "AUC Score: 0.6843\n",
      "Precision: 0.16379937000242306\n",
      "Recall: 0.6739780658025922\n",
      "F1 Score: 0.26354775828460036\n",
      "0    32354\n",
      "1    16390\n",
      "Name: TARGET, dtype: int64\n",
      "Prediction Accuracy: 0.8275\n",
      "AUC Score: 0.6555\n",
      "Precision: 0.22333267365921236\n",
      "Recall: 0.45004985044865403\n",
      "F1 Score: 0.2985252298128431\n",
      "0    41019\n",
      "1     7725\n",
      "Name: TARGET, dtype: int64\n",
      "Prediction Accuracy: 0.8489\n",
      "AUC Score: 0.6477\n",
      "Precision: 0.2442027253167583\n",
      "Recall: 0.4073778664007976\n",
      "F1 Score: 0.3053583439204843\n",
      "0    42569\n",
      "1     6175\n",
      "Name: TARGET, dtype: int64\n",
      "---515.5551784038544 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "Mod_performance_1 = smpl_sub.copy()\n",
    "Mod_performance_1.iloc[:,1] = modelperformancenewdata(ridge_mod, x_smote, y_smote, x_test, y_test, data_test)\n",
    "\n",
    "print(Mod_performance_1['TARGET'].value_counts())\n",
    "Mod_performance_1.to_csv(\"Model_pred_1.csv\", index = False, header = True)\n",
    "\n",
    "Mod_performance_2 = smpl_sub.copy()\n",
    "Mod_performance_2.iloc[:,1] = modelperformancenewdata(xgb_mod1, x_under, y_under, x_test, y_test, data_test)\n",
    "\n",
    "print(Mod_performance_2['TARGET'].value_counts())\n",
    "Mod_performance_2.to_csv(\"Model_pred_2.csv\", index = False, header = True)\n",
    "\n",
    "Mod_performance_3 = smpl_sub.copy()\n",
    "Mod_performance_3.iloc[:,1] = modelperformancenewdata(xgb_mod1, x_over, y_over, x_test, y_test, data_test)\n",
    "\n",
    "print(Mod_performance_3['TARGET'].value_counts())\n",
    "Mod_performance_3.to_csv(\"Model_pred_3.csv\", index = False, header = True)\n",
    "\n",
    "print(\"---%s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three models had auc scores of 0.6477 - 0.6843. The top model was the ridge regression when using smote sampling. Total time to train and test these models was just under 9 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---0.18550324440002441 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "Ems_performance =  smpl_sub.copy()\n",
    "Ems_performance['TARGET'] = (Mod_performance_1['TARGET'] + Mod_performance_2['TARGET'] + Mod_performance_3['TARGET']) / 3\n",
    "Ems_performance.to_csv(\"ems_pred.csv\", index = False, header = True)\n",
    "print(\"---%s seconds ---\" % (time.time() - start_time)) # Model runtime is dependent on the run time of the other three models. It will be lond due to waiting on the other models.\n",
    "# Total runtime is just under 9 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary:\n",
    "The task involved the development of a classification model for predicting loan applicant default likelihood. The provided dataset included various credit-related details for each application. Data preprocessing involved outlier removal, mean value imputation for missing columns, and conversion of categorical variables into dummy variables. After preparing the final dataset, it was divided into training and testing sets.To train the models, K-fold cross-validation was employed, utilizing five different classification methods to create a total of ten models. In order to tackle class imbalance in the data, three sampling methods were implemented. The top three models were selected based on their performance, assessed using the area under the curve (AUC) as the metric. These top models comprised two XGboosted models and one Ridge Regression linear model, with AUC scores ranging from 0.6477 to 0.6843.\n",
    "The ultimate model created was an ensemble model, combining the top three models along with their specific sampling techniques. This ensemble model achieved the highest score among the four models submitted to Kaggle, with a score of 0.71609. We recommend that Home Credit adopts this combined approach for future application evaluations, as it outperforms the majority response classification by a 20% margin. By implementing this model, Home Credit can make more accurate predictions regarding the suitability of loaning money to applicants, based on their prior credit history."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
